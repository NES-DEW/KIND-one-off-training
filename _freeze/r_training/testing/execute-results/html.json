{
  "hash": "d4004e2afa126e3f2f0c44f6a05a232e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Testing R code\"\ndate: \"2024-08-07\"\nexecute: \n  echo: true\n  eval: true\n  freeze: auto\noutput: \"markup\"\ncategories: [R, intermediate]\n---\n\n\n::: {.callout-note collapse=\"false\" appearance=\"default\" icon=\"true\"}\n## Session materials\n\n-   [all materials {{< iconify ph:file-zip size=2x >}}](src/testing.zip)\n-   slides [{{< iconify ph:file-html size=2x >}} html](src/testing.html) / [{{< iconify ph:file-pdf size=2x >}} pdf](src/testing.pdf)\n:::\n\n\n\nNo feedback found for this session\n\n\n\n## Welcome\n\n-   this session is for ðŸŒ¶ðŸŒ¶ intermediate users\n-   you'll need R + Rstudio / Posit Workbench / posit.cloud to follow along\n\n## Session outline\n\n+ introduction: why test?\n+ informal testing\n+ unit testing\n    + introduction - why automate your tests?\n    + `testthat` walkthrough\n\n## Introduction: why test?\n\n+ code goes wrong       \n    + functions change\n    + data sources change\n    + usage changes\n+ testing guards against the commonest problems\n+ that makes for more reliable code\n    + more reliable code opens the way to nicer development patterns\n\n## A note\n\n+ most discussions about testing come about as part of package development \n    + we'll avoid that area here, but please see the [three excellent chapters in the R packages book](https://r-pkgs.org/testing-basics.html)  for guidance\n    + we'll also steer clear of Shiny/Rmarkdown/Quarto, as things can be a bit more tricky to test there\n+ we also won't talk about debugging here (although do look out for the future training session on that)\n\n## Informal testing\n\n+ a real-world example: Teams transcripts\n+ Teams transcripts can be very useful data-sources\n+ but they're absolutely horrible to work with:\n\n\n\n```{.txt}\nWEBVTT\nQ1::>\n00:00:00.000 --> 00:00:14.080\n<v Brendan Clarke> this was the first question in the transcript\n\n00:00:14.080 --> 00:00:32.180\n<v Someone Else> then someone replied with this answer\n\nQ2::>\n00:00:32.180 --> 00:00:48.010\n<v Brendan Clarke> then there was another question\n\n00:00:48.010 --> 00:00:58.010\n<v Someone Else> and another tedious response\n```\n\n\n+ imagine that you've written a (horrible) Teams transcript parser:\n+ how would you test this code to make sure it behaves itself?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile <- \"data/input.txt\"\n\nreadLines(file) |>\n    tibble::as_tibble() |>\n    dplyr::filter(!value == \"\") |>\n    dplyr::filter(!value == \"WEBVTT\") |>\n    dplyr::mutate(question = stringr::str_extract(value, \"^(Q.*?)::>$\")) |>\n    tidyr::fill(question, .direction = 'down') |>\n    dplyr::filter(!stringr::str_detect(value,  \"^(Q.*?)::>$\")) |>\n    dplyr::mutate(ind = rep(c(1, 2),length.out = dplyr::n())) |>\n    dplyr::group_by(ind) |>\n    dplyr::mutate(id = dplyr::row_number()) |>\n    tidyr::spread(ind, value) |>\n    dplyr::select(-id) |>\n    tidyr::separate(\"1\", c(\"start_time\", \"end_time\"), \" --> \") |>\n    tidyr::separate(\"2\", c(\"name\", \"comment\"), \">\") |>\n    dplyr::mutate(source = stringr::str_remove_all(file, \"\\\\.txt\"),\n           name = stringr::str_remove_all(name, \"\\\\<v \"), \n           comment = stringr::str_trim(comment), \n           question = stringr::str_remove_all(question, \"::>\")) |>\n    knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|question |start_time   |end_time     |name           |comment                                       |source     |\n|:--------|:------------|:------------|:--------------|:---------------------------------------------|:----------|\n|Q1       |00:00:00.000 |00:00:14.080 |Brendan Clarke |this was the first question in the transcript |data/input |\n|Q1       |00:00:14.080 |00:00:32.180 |Someone Else   |then someone replied with this answer         |data/input |\n|Q2       |00:00:32.180 |00:00:48.010 |Brendan Clarke |then there was another question               |data/input |\n|Q2       |00:00:48.010 |00:00:58.010 |Someone Else   |and another tedious response                  |data/input |\n\n\n:::\n:::\n\n\n+ we could change the inputs, and look at the outputs\n    + so twiddle our input file, and manually check the output\n+ maybe we could also change the background conditions\n    + change the R environment, or package versions, or whatever\n+ but that gets tedious and erratic very quickly\n\n## `testthat`\n\n+ Unit testing = automated, standardised, testing\n+ the best place to start is with `testthat`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(testthat)\n```\n:::\n\n\n+ [helpful man pages](https://testthat.r-lib.org/)\n+ [nice vignette](https://combine-australia.github.io/r-pkg-dev/testing.html) \n+ [more ambitious guide to ad hoc testing with `testthat`](https://batteriesnotincluded.rbind.io/post/2017/08/ad-hoc-testing/)\n\n### First steps with `testthat`\n\n+ built for R package developers\n+ but readily usable for non-package people\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest passed ðŸŒˆ\n```\n\n\n:::\n:::\n\n\n### Functions and `testthat`\n\n+ `testthat` works best when you're testing functions\n+ functions in R are easy:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfunction_name <- function(arg1 = default1, arg2 = default2){\n     arg1 * arg2 # using our argument names\n}\n```\n:::\n\n\n\n\nor include the body inline for simple functions:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfunction_name <- function(arg1 = default1, arg2 = default2) arg1 * arg2\n```\n:::\n\n\n### Transform your code into functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmulto <- function(n1, n2){\n  n1 * n2\n}\n```\n:::\n\n\n### Test your function\n\n+ then test. We think that `multo(2,2)` should equal 4, so we use:\n    + `test_that()` to set up our test environment\n    + `expect_equal()` inside the test environment to check for equality\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# then run as a test\n\ntest_that(\"multo works with 2 and 2\", {\n    expect_equal(multo(2, 2), 4)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest passed ðŸ˜€\n```\n\n\n:::\n:::\n\n\n### Raise your expectations\n\n+ we can add more expectations\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_that(\"multo works in general\", {\n    expect_equal(multo(2, 2), 4)\n    expect_identical(multo(2,0.01), 0.02)\n    expect_type(multo(0,2), \"double\")\n    expect_length(multo(9,2), 1)\n    expect_gte(multo(4,4), 15)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest passed ðŸ¥³\n```\n\n\n:::\n:::\n\n\n### Equal and identical\n\n+ beware the [floating point error](https://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n3 - 2.9\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1\n```\n\n\n:::\n\n```{.r .cell-code}\n3 - 2.9 == 0.1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n+ happily, there's a sufficiently sloppy way of checking equality:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_that(\"pedants corner\", {\n  expect_equal(multo(2, 0.01), 0.020000001)\n  expect_identical(multo(2, 0.01), 0.02)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest passed ðŸ˜¸\n```\n\n\n:::\n:::\n\n\n## Testing several values\n\n+ if you want to work with vectors, there are a number of tools for checking their contents:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rownames(as.matrix(eurodist, labels=TRUE)) # odd built in dataset\n\ntest_that(\"check my vec\", {\n    expect_equal(x[1:2], c(\"Athens\", \"Barcelona\"))\n})    \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest passed ðŸ˜€\n```\n\n\n:::\n:::\n\n\n+ you can get **much** more fancy with a bit of set theory (not really [set theory](https://plato.stanford.edu/entries/set-theory/basic-set-theory.html)):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- x\n\ntest_that(\"check my vec sets\", {\n    expect_success(expect_setequal(x, y)) # all x in y\n    expect_failure(expect_mapequal(x, y)) # same names, y is proper subset x) # all x in y)\n    show_failure(expect_contains(x[1:19], y)) # y proper subset x)\n    expect_success(expect_in(x, y)) # x proper subset y\n})    \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFailed expectation:\nx[1:19] (`actual`) doesn't fully contain all the values in `y` (`expected`).\n* Missing from `actual`: \"Stockholm\", \"Vienna\"\n* Present in `actual`:   \"Athens\", \"Barcelona\", \"Brussels\", \"Calais\", \"Cherbourg\", \"Cologne\", \"Copenhagen\", \"Geneva\", \"Gibraltar\", ...\n\nTest passed ðŸŒˆ\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- sample(x, length(x)-2)\n\ntest_that(\"check my vec sets\", {\n    expect_failure(expect_setequal(x, y)) # all x in y\n    expect_failure(expect_mapequal(x, y)) # same names, y is proper subset x) # all x in y)\n    expect_success(expect_contains(x, y)) # y proper subset x)\n    expect_failure(expect_in(x, y)) # x is a proper subset y\n})    \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest passed ðŸ˜€\n```\n\n\n:::\n:::\n\n\n### Testing tibbles\n\n+ because most of the tests are powered by [`waldo`](https://www.tidyverse.org/blog/2020/10/waldo/), you shouldn't have to do anything fancy to test on tibbles:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\n\nmy_pengs <- penguins\n\ntest_that(\"penguin experiments\", {\n    expect_equal(my_pengs, penguins)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest passed ðŸ¥³\n```\n\n\n:::\n:::\n\n\n### Types and classes etc\n\n+ one *massive* corollary to that: if you don't do a lot of base-R, expect a fiercely stringent test of your [understanding of types and classes](https://adv-r.hadley.nz/base-types.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntypeof(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"list\"\n```\n\n\n:::\n\n```{.r .cell-code}\nclass(penguins) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n```\n\n\n:::\n\n```{.r .cell-code}\nis.object(names(penguins)) # vectors are base types\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n\n```{.r .cell-code}\nattr(names(penguins), \"class\") # base types have no class\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNULL\n```\n\n\n:::\n\n```{.r .cell-code}\nis.object(penguins) # this is some kind of object\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nattr(penguins, \"class\") # so it definitely does have a class\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n```\n\n\n:::\n:::\n\n\n### Tibble tests\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_that(\"penguin types\", {\n    expect_type(penguins, \"list\")\n    expect_s3_class(penguins, \"tbl_df\")\n    expect_s3_class(penguins, \"tbl\")\n    expect_s3_class(penguins, \"data.frame\")\n    expect_type(penguins$island, \"integer\")\n    expect_s3_class(penguins$island, \"factor\")\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest passed ðŸ¥‡\n```\n\n\n:::\n:::\n\n\n+ there's also an `expect_s4_class` for those with that high clear mercury sound ringing in their ears\n\n### Last tip\n\n+ you can put bare expectations in pipes if you're looking for something specific\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |>\n  expect_type(\"list\") |>\n  dplyr::pull(island) |>\n  expect_length(344)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}